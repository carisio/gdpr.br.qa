{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1504f1bb-dfc2-4d92-bc2b-842c919b2d87",
   "metadata": {},
   "source": [
    "# 1. Prompt the LLM with only the question (without any context and with LGPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d666c315-7cda-4b79-b621-6db3ec6cd886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7473cce9-cbf8-466d-87a7-ec7bcde7ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "KEY OpenAI ········\n",
      "KEY DeepSeek ········\n",
      "KEY Groq ········\n",
      "KEY Maritaca ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_KEY = getpass(\"KEY OpenAI\")\n",
    "DEEPSEEK_KEY = getpass(\"KEY DeepSeek\")\n",
    "GROQ_API = getpass(\"KEY Groq\")\n",
    "SABIA_API = getpass(\"KEY Maritaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0531056e-7a63-4e97-842c-e10da4873ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"dataset/gdpr-br-qa.jsonl\"\n",
    "gdpr_br_file = \"lgpd/lgpd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a03152-b606-4bc1-bfb2-500b1890f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir e ler o conteúdo do arquivo\n",
    "with open(gdpr_br_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    gdpr_br = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed3fbdfd-a704-4438-8d18-cbaf0ee3bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_without_context = \"\"\"\n",
    "Seu papel é resolver questões sobre a Lei Geral de Proteção de Dados Pessoais (LGPD) do Brasil. Para isso, você deverá obedecer às seguintes regras:\n",
    "\n",
    "1. O usuário irá te passar uma questão que você deverá responder.\n",
    "2. Essa questão pode ser de dois tipos. Pode ser de múltipla escolha, situação em que você deverá avaliar o enunciado e decidir por uma das alternativas apresentadas, ou pode ser de <CERTO> ou <ERRADO>, situação em que você deverá avaliar se o enunciado está correto ou não. O que diferencia as questões é a presença ou ausência de alternativas. Se tiver alternativas, é de múltipla escolha. Se não houver, é de <CERTO> ou <ERRADO>\n",
    "3. A sua resposta deve ser direta, sem nenhuma marcação (pontuação, aspas ou qualquer outro caractere além da resposta).\n",
    "3.1. Se for uma questão de múltiplica escolha, indique apenas a letra que responde ao enunciado.\n",
    "3.2. Se for uma questão de <CERTO> ou <ERRADO> responda com \"Certo\" (para certo) ou \"Errado\" para errado.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5fbe6a3-8741-4178-bf82-909129470607",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_with_gdpr = \"\"\"\n",
    "Considere a Lei Geral de Proteção de Dados Pessoais (LGPD), transcrita abaixo:\n",
    "\n",
    "[[INICIO LGPD]]\n",
    "{gdpr}\n",
    "[[FIM LGPD]]\n",
    "\n",
    "Seu papel é resolver questões sobre a LGPD. Para isso, você deverá obedecer às seguintes regras:\n",
    "\n",
    "1. O usuário irá te passar uma questão que você deverá responder.\n",
    "2. Essa questão pode ser de dois tipos. Pode ser de múltipla escolha, situação em que você deverá avaliar o enunciado e decidir por uma das alternativas apresentadas, ou pode ser de <CERTO> ou <ERRADO>, situação em que você deverá avaliar se o enunciado está correto ou não. O que diferencia as questões é a presença ou ausência de alternativas. Se tiver alternativas, é de múltipla escolha. Se não houver, é de <CERTO> ou <ERRADO>\n",
    "3. A sua resposta deve ser direta, sem nenhuma marcação (pontuação, aspas ou qualquer outro caractere além da resposta).\n",
    "3.1. Se for uma questão de múltiplica escolha, indique apenas a letra que responde ao enunciado.\n",
    "3.2. Se for uma questão de <CERTO> ou <ERRADO> responda com \"Certo\" (para certo) ou \"Errado\" para errado.\n",
    "\"\"\".format(gdpr=gdpr_br).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "106dcb86-2923-466e-820e-48109893d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros dos modelos\n",
    "\n",
    "NO_CTX_GPT_4o_MINI = {\n",
    "    \"EXPERIMENT_NAME\": \"no_context_gpt-4o-mini-2024-07-18\",\n",
    "    \"CLIENT\": OpenAI(api_key=OPENAI_KEY, base_url=None),\n",
    "    \"MODEL\": \"gpt-4o-mini-2024-07-18\",\n",
    "    \"SYSTEM_MSG\": system_msg_without_context\n",
    "}\n",
    "\n",
    "NO_CTX_DEEPSEEK_CHAT = {\n",
    "    \"EXPERIMENT_NAME\": \"no_context_deepseek-chat\",\n",
    "    \"CLIENT\": OpenAI(api_key=DEEPSEEK_KEY, base_url=\"https://api.deepseek.com\"),\n",
    "    \"MODEL\": \"deepseek-chat\",\n",
    "    \"SYSTEM_MSG\": system_msg_without_context\n",
    "}\n",
    "\n",
    "NO_CTX_LLAMA3_3 = {\n",
    "    \"EXPERIMENT_NAME\": \"no_context_llama-3.3-70b-versatile\",\n",
    "    \"CLIENT\": Groq(api_key=GROQ_API),\n",
    "    \"MODEL\": \"llama-3.3-70b-versatile\",\n",
    "    \"SYSTEM_MSG\": system_msg_without_context\n",
    "}\n",
    "\n",
    "NO_CTX_SABIA3 = {\n",
    "    \"EXPERIMENT_NAME\": \"no_context_sabia-3-2024-12-11\",\n",
    "    \"CLIENT\": OpenAI(api_key=SABIA_API, base_url=\"https://chat.maritaca.ai/api\"),\n",
    "    \"MODEL\": \"sabia-3-2024-12-11\",\n",
    "    \"SYSTEM_MSG\": system_msg_without_context\n",
    "}\n",
    "\n",
    "CTX_GPT_4o_MINI = {\n",
    "    \"EXPERIMENT_NAME\": \"with_gdpr_gpt-4o-mini-2024-07-18\",\n",
    "    \"CLIENT\": OpenAI(api_key=OPENAI_KEY, base_url=None),\n",
    "    \"MODEL\": \"gpt-4o-mini-2024-07-18\",\n",
    "    \"SYSTEM_MSG\": system_msg_with_gdpr\n",
    "}\n",
    "\n",
    "CTX_DEEPSEEK_CHAT = {\n",
    "    \"EXPERIMENT_NAME\": \"with_gdpr_deepseek-chat\",\n",
    "    \"CLIENT\": OpenAI(api_key=DEEPSEEK_KEY, base_url=\"https://api.deepseek.com\"),\n",
    "    \"MODEL\": \"deepseek-chat\",\n",
    "    \"SYSTEM_MSG\": system_msg_with_gdpr\n",
    "}\n",
    "\n",
    "CTX_LLAMA3_3 = {\n",
    "    \"EXPERIMENT_NAME\": \"with_gdpr_llama-3.3-70b-versatile\",\n",
    "    \"CLIENT\": Groq(api_key=GROQ_API),\n",
    "    \"MODEL\": \"llama-3.3-70b-versatile\",\n",
    "    \"SYSTEM_MSG\": system_msg_with_gdpr\n",
    "}\n",
    "\n",
    "CTX_SABIA3 = {\n",
    "    \"EXPERIMENT_NAME\": \"with_gdpr_sabia-3-2024-12-11\",\n",
    "    \"CLIENT\": OpenAI(api_key=SABIA_API, base_url=\"https://chat.maritaca.ai/api\"),\n",
    "    \"MODEL\": \"sabia-3-2024-12-11\",\n",
    "    \"SYSTEM_MSG\": system_msg_with_gdpr\n",
    "}\n",
    "\n",
    "EXPERIMENTS = [NO_CTX_GPT_4o_MINI, NO_CTX_LLAMA3_3, NO_CTX_DEEPSEEK_CHAT, NO_CTX_SABIA3,\n",
    "               CTX_GPT_4o_MINI, CTX_LLAMA3_3, CTX_DEEPSEEK_CHAT, CTX_SABIA3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c3d5d5-cbb2-4b91-986a-f9ecafaf97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_json(dataset_file, lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65589d13-c71a-4547-900e-4fef42b7c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to represent the experiment results\n",
    "for experiment in EXPERIMENTS:\n",
    "    experiment_name = experiment['EXPERIMENT_NAME']\n",
    "    llm_model = experiment['MODEL']\n",
    "\n",
    "    # Create columns for the experiment results\n",
    "    if experiment_name not in df_dataset.columns:\n",
    "        df_dataset[experiment_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1d6e9f-4bbf-4969-8edc-e5aa47680709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_llm(experiment, formatted_question):\n",
    "    llm_model = experiment['MODEL']\n",
    "    client = experiment['CLIENT']\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": experiment['SYSTEM_MSG']},\n",
    "            {\"role\": \"user\", \"content\": formatted_question},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3741cc0-fd6f-4d30-ab16-36ec567f23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: no_context_gpt-4o-mini-2024-07-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 9375.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: no_context_llama-3.3-70b-versatile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 16355.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: no_context_deepseek-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 12926.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: no_context_sabia-3-2024-12-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 17103.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: with_gdpr_gpt-4o-mini-2024-07-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 18830.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: with_gdpr_llama-3.3-70b-versatile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 700/700 [33:03<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: with_gdpr_deepseek-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 84244.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: with_gdpr_sabia-3-2024-12-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 52253.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for experiment in EXPERIMENTS:\n",
    "    experiment_name = experiment['EXPERIMENT_NAME']\n",
    "    llm_model = experiment['MODEL']\n",
    "    \n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "       \n",
    "    for index, row in tqdm(df_dataset.iterrows(), total=len(df_dataset)):           \n",
    "        # Run only if we don't have the results\n",
    "        if pd.isna(row[experiment_name]) or row[experiment_name] == '':\n",
    "            df_dataset.at[index, experiment_name] = get_answer_llm(experiment, row['Formatted Question'])\n",
    "            \n",
    "            # Update the dataset with the results\n",
    "            with open(dataset_file, 'w', encoding='utf-8') as f:\n",
    "                df_dataset.to_json(f, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56945c4c-ea49-4624-8b02-782965a460e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: no_context_gpt-4o-mini-2024-07-18. Avg: 0.7571428571428571.\n",
      "Experiment: no_context_llama-3.3-70b-versatile. Avg: 0.7314285714285714.\n",
      "Experiment: no_context_deepseek-chat. Avg: 0.85.\n",
      "Experiment: no_context_sabia-3-2024-12-11. Avg: 0.8671428571428571.\n",
      "Experiment: with_gdpr_gpt-4o-mini-2024-07-18. Avg: 0.8328571428571429.\n",
      "Experiment: with_gdpr_llama-3.3-70b-versatile. Avg: 0.27714285714285714.\n",
      "Experiment: with_gdpr_deepseek-chat. Avg: 0.9414285714285714.\n",
      "Experiment: with_gdpr_sabia-3-2024-12-11. Avg: 0.78.\n"
     ]
    }
   ],
   "source": [
    "# Show results for each experiment\n",
    "for experiment in EXPERIMENTS:\n",
    "    experiment_name = experiment['EXPERIMENT_NAME']\n",
    "    avg = (df_dataset['Answer'] == df_dataset[experiment_name]).mean()\n",
    "    print(f\"Experiment: {experiment_name}. Avg: {avg}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
